<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Splatography: Sparse multi-view dynamic Gaussian Splatting for film-making challenges</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.JPG">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Splatography: Sparse multi-view dynamic Gaussian Splatting for film-making challenges
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://azzarelli.github.io/" target="_blank">Adrian Azzarelli</a>,</span>
                <span class="author-block">
                  <a href="https://pui-nantheera.github.io/" target="_blank">Nantheera Anantrasirichai</a>,</span>
                  <span class="author-block">
                <a href="https://david-bull.github.io/" target="_blank"> David R Bull</a>
              </span>
  
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="static/pdfs/suppmat.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Supp. Material</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="link_to_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <span style="font-style: italic;text-align: center;"> 
              Below are some examples of creative applications using our method for sparse view 3-D 
              reconstruction, followed by an overview of our work and finally the various results, benchmarks and
              renders from the paper.
            </span>
<br>
            <br>
        Extended Abstract:
          <br>
        <div class="content has-text-justified">
          <p>
Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning 
to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits 
state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the 
canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t=0. Each 
representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different
 parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic 
 features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically
  dimmer and less dynamic so only changes in point position are learned.
Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3
 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also 
 produces segmented dynamic reconstructions including transparent and dynamic textures. 

            
           
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
           <h2 class="subtitle has-text-centered">
            Audio-visual VFX for music
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/showcase.mp4"
            type="video/mp4">
          </video>
        </div>

        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Combining various Gaussian Splatting VFX 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/piano_vfx.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Ripple, Fade out, Bloom, Chromatic Abberation, Undulate out, Fly Up 
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed work:</h2>

      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <br>
      <img src="static/images/main.png" alt="~Missing Image~"/>

      <h2 class="subtitle">
This paper's primary focus is to tackle SV3D reconstruction challenges by:  <br>    
<span style="margin-left: 30px;">(1) Proposing a new dynamic scene representation that 
disentangles the foreground and background to deal with point-importance</span>
<br>
<span style="margin-left: 30px;">(2) Developing a new strategy for training canonical representations to deal with poor initialization.</span>
<br>
<br>
Linking this work to filmmaking, we leverage foreground-background separation to 
apply background-specific constraints based on filmmaking practices; discussed in Introduction. This aims to suppress
 the reconstruction artifacts associated with background and foreground reconstruction, which we indentify in the above figure.
We also place emphasis on reconstructing RTD textures as they are more common in filmmaking 
  than most other practices.
   </h2>
    </div>
  </div>
</section>



<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <span class="link-block">
          <a href="https://vivo-bvicr.github.io/index.html" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Vivo 3-D Entertainment Scenes</span>
        </a>
      </span>
      <br> *videos downsampled to 720p for web-viewing

      </div>
    </div>
  </div>
</section>
<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
           <h2 class="subtitle has-text-centered">
            Bassist Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            
            <source src="static/videos/bassist.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Ground Truth | 4D-GS | STG | SC-GS | Ours
            <br> Small foreground assets are challenging for SotA to reconstruct as the background consumes a lot
            of the image/render and volumetric scene spaces. By disentangling foreground and background we overcome
            this issue.
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Pony Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/pony.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Ground Truth | 4D-GS | STG | SC-GS | Ours
            <br> Large foreground assets are easier for SotA to reconstruct (compared with Bassist) but still miss many
            fine details.
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Piano Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/piano.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            4D-GS | STG | Ours | Ours (Foreground Only)
            <br> We disentangle the scene using foreground-background separation.
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Fruit Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/fruit.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            4D-GS | Ours | Ours (Foreground Only)
            <br> We supress various over and under reconstruction errors. Check out the background at time 30-40s!
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Curling Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/curling.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            4D-GS | STG | Ours | Ours (Foreground Only)
            <br> There are still clear issues with SV3D. Notably, dealing with large motions that cause large temporal occlusions. This
            affects both the renders (check out 4DGS and Ours) as well as the final segmented model. This is an issue with disentangling 
            view-dependant color using Spherical Harmonics (honest SH is terrible for dynmaic scenes). Behind the scenes, we tested a simple
            view-dependant MLP instead of SH and its much easier to constrain... 
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <span class="link-block">
          <a href="https://github.com/facebookresearch/Neural_3D_Video" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Dy-NeRF 2.5-D Cooking Scenes</span>
        </a>
      </span>

      </div>
    </div>
  </div>
</section>
<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Flame Salmon Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/salmon.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            GT | 4D-GS | W4D-GS | Ours 
            <br> GT | STG | ITGS | Ours (Foreground Only)
            <br> ...
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Flame Steak Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/steak.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            GT | 4D-GS | W4D-GS | Ours 
            <br> GT | STG | ITGS | Ours (Foreground Only)
            <br> ...
          </h2>
        </div>
        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Cook Spinach Test Videos 
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/spinach.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            GT | 4D-GS | W4D-GS | Ours 
            <br> GT | STG | ITGS | Ours (Foreground Only)
            <br> ...
          </h2>
        </div>
  
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Novel-view Renders</h2>
        <!-- <p class="subtitle has-text-centered" style="font-style: italic;font-weight: 100; color: brown;">
            Note that for the Vivo scenes we do not have novel views for other models because the novel-view trajectory is dense with floaters 
            - check test views. Read the paper to undestand why...
        </p> -->
      </div>
    </div>
  </div>
</section>
<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
           <h2 class="subtitle has-text-centered">
            Slow-Motion Flame Salmon
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/salmon_nvs.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            4D-GS | Ours | Ours (Foreground Only)
          </h2>
        </div>

        <div class="item item-video2">
          <h2 class="subtitle has-text-centered">
            Slow-Motion Flame Steak
          </h2>
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/steak_nvs.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            4D-GS | W4D-GS | Ours | Ours (Foreground Only)
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>



<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablations (+ Supplementary)</h2>
        <p class="subtitle has-text-centered" style="font-style: italic;font-weight: 100; color: black;">
          As per the ablations in the paper we compare with: 
        </p>
        <p class="subtitle" style="font-style: italic;font-weight: 100; color: black;">
          (1) Unified Λ as in 4DGS, (2) Canonical training as in all other plane-based methods, (3) 3DGS/4DGS densification strategy,
            (4) Temporal Opacity function in STG.
        </p>
        <!-- <p class="subtitle" style="font-style: italic;font-weight: 100; color: brown;">
          TLDR; The comparisons are made w.r.t. prior work as all parts of the ablation have been investigated to some capacity (this statement does not insinuate the prior
            works "deeply" investigate these parts of dynamic GS pipelines). We show through the following videos that
            our separating foreground/background is beneficial, that the vanilla canonical training approach is less adept than our approach, that vanilla densification is
            destructive for SV3D paradigms and that our modification of the STG temporal opacity function works.
        </p> -->
            


        </p>
      </div>
    </div>
  </div>
</section>

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/piano_test_ablations.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
             Ours | Unified Λ | 4DGS Canonical Training | 4DGS Densification | STG Temporal Opacity 
            <br>
            Test views on the Piano Scene
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/pony_ablations.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Ours | Unified Λ | 4DGS Canonical Training | 4DGS Densification | STG Temporal Opacity 
            <br>
            Novel views on the Pony Scene
          </h2>
        </div>

        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/piano_ablations.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">            
            Ours | Unified Λ | 4DGS Canonical Training | 4DGS Densification | STG Temporal Opacity 
            <br>
            Novel views on the Piano Scene
          </h2>
        </div>

        
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Notes for Future Work</h2>

      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <span  style="color: brown; font-style: italic;">We compiled some notes for anyone interested in exploring future works relating to dynamic GS reconstruction. This is written in essay/conversation form. For any queries
          please email ....
        </span> 
          <br>
        <div class="content has-text-justified">
          <p>
An area for research that has recently caught our gaze is <a href="https://arxiv.org/abs/2412.06299">SaRO-GS</a>, which tackles the issue of sampling hex-planes 
with multi-scale grid resolutions - we use a single resolution while other methods fuse features via concatenations, sampled from a set of multi-scale feature planes. 
This also encouraged an internal debate for us between using billinear and nearest neighbour for grid sampling. 
For any temporal event (concerning the XT, YT and ZT planes) the temporal axis' resolution 
is typically set to half the number of frames. This allows multiple samples (at different time steps) to inhabit the same grid-cell at various intervals along the cell's temporal axis. 
Thus, sampling this axis with linear interpolation (billinearly in 2-D) means that we get a relatively smooth transition between timestep leading to a nearly-smooth temporal feature space - this is 
good for us. However, for space-only features (concerning XY, XZ and YZ planes) we have to think a bit harder. The obvious point to make is that proximal points 
(in our canonical space) share different portions of the same features due to the bilieanr interpolation operation, which is good for points that pertain to the same 
object or limb for humans. However, what if two nearby points pertain to different objects? For small grid resolutions this becomes a problem, yet with large grid 
resolutions objects pertaining to the same body no longer share features smoothly. Furthermore, in sparse-view settings, the intial geometry may not be viable so proximal points pertaining
to different objects may be initially mixed together (before hopefully later separating).
Thus, we have to think a bit harder. SaRO-GS explores a solution that essentially simulates subsampling a canonical Gaussian based the covariance of a canonical point splatted onto 
the hex-planes. They also do some work to avoid large canonical Gaussians having influence on the high resolution grids (reminder that hex-planes fuse features sampled from multi-scale planes) 
by restricting the subsampling of multi-scale planes based on Gaussian size. Their method works - we use something similar in our approach, whereby we uniformly subsample 12 additional points based on fixed 
Mahalanobis distances frm the center, based on the canonical rotation and scale parameters. As we use a single feature-plane scale, we do not enforce multi-scale filtering based on point scale as with SaRO-GS,
though this perhaps isn't as necessary as we may think considering that the impact of distance sub-samples, sampled by a large canonical point, impact the final feature less as it accounts for 1/13th of the final
feature values, such that some fine features can be spatially preserved. Still our solution is far from ideal and requires further investigation.
<br>
<br>
Tied to this is the interpretation of the canonical Gaussian and deformation field as entangled components. We note that for most hex-plane GS methods (not ours), the canon is expected to learn the 
optimal position of points for sampling G, as well as acting as a basis vector such that x' = x + deformation. In our approach, we try to tie the canon to t=0 through the proposed canonical training 
strategy. Then, when training the dynamic scene, the canonical points should organize themselves in the same way as 4DGS and other approaches. Though, due to the initialisation, there is still some 
link tying the canon to t=0. This doesn't necessarily presents a dilemma for us to investigate, though it does get our minds thinking about better and less entangled approaches to treating the canon 
w.r.t the dynamic 
field. One idea we would like to try out is to treat deformation feature outputs (the changes in color, rotation and position) as being normalize between 0 and 1 (e.g. using a Cosine activation n the defomation 
decoder outputs). 
Considering that both rotation and 
color already lead to values between 0 and 1 (either after decoding or during the SH to RGB operation), we think there may be benefit in forcing point position to adhere to similar conditions. This may 
level the importance of various dynamic Gaussian parameters during gradient backpropagation. As one may expect a change in position (w.r.t the scene scale) may be much larger than a change in color or 
rotation so backpropagation will affect color and positional changes non-linearly. This would make learning position challenging and as we know from countless reports - dynamic GS does have an underlying 
issue learning scene geometry. By forcing x' = deformation, we may need to rescale to have x' = k * deformation, such that the final position can move more than 1 unit in space. Considering that 
for most filmmaking applications dynamic motion and effects are contained within the stationary stage, re-scaling may be as simple as multiplying the normalize positional by the stage's extent. These
measurements are easy to generate by, for example, analysing the initial point cloud.   
<br>
<br>

This may also open up new doors. We tinkered with the idea of a deformation field that did not update the initial gaussian G but instead replaced it, such that x' = deformation instead of 
x' = x + deformation. This allows us to simplify the aforementioned interpretations of the canon and deformation field, as the canon acts only to sample the deformation field and the deformation 
field no longer acts to predict residuals but instead directly produces the Gaussian field at t. While this didnt work, it also didn't completely fail and has pushed us to think about other potential 
avenues for re-interpreting the relatively confusing hex-plane field representations. We believe the ideal solution lies with a grounded interpretations that allows for disentangling visual changes 
(color) from geometric changes (position and rotation). We experimented on learning a different set of (XT, yT and ZT) planes for the color feature and found little quantitative change but some very 
small visual improvements - which are really only evident under a microscope so do not motivate any paper. Along the same lines, we also looked at modelling the temporal opacity parameters (h, omega and mu) 
using the (XY, XZ and YZ) planes and decoding the feature from the three space-only planes. We found more promising results, but in the Splatography paper it made more sense to leave them as explicit 
parameters, as intended from STG, as we wanted a direct coparison with our proposed modifications to the temporal opacity function.
<br>
<br>

While its unexplored, this later attempt at research does indicate that benefit can come from re-interpreting the space-only and space-time planes, such that, again, visual and geometric changes c an be 
disentangled. One approach we rather like is the STG method, that assumed no visual change until rendering, whereby the render-er is assumed to be incontrol of the dynamic visual changes. This method, 
however, treats all dynamic Gaussian parameters as explicit and disconnected components so working this into a hex-plane method could be rather challenging - though perhaps using the hex-planes to only 
model geometry and using 2-D (1-to-1) feature-to-image decoders (as in STG) to model view-dependant and visual changes could prove a powerful solution. The only caveat being that the STG rendering 
approach is heavy and limited to ours/your ability to use CUDA code.
<br>
<br>

          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{...}</code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- End image carousel -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to that page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
