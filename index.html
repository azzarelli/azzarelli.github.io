<html>
  
  <meta charset="UTF-8">
  <meta content="width=device-width, initial-scale=1" name="viewport" />

<head>
  <link rel="preload" href="scripting/home.js" as="script">

  <link href="./css/foreground.css" type="text/css" rel="stylesheet">
  <link href="./css/background.css" type="text/css" rel="stylesheet">
  
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="js/dropdown.js"></script>

</head>


<body style="padding-right: 0px;">
  <script src="jquery-3.4.1.js"></script>

<script src="scripting/home/script.js"></script>
<script src="scripting/home/javaid.js"></script>

  <div id="foreground">

    <div class="project-container" style="padding-bottom:0px;display: flex;">
      <img class="photo" src="css/me.png">

      <div class="introduction-text-container">
        <p class="introduction-text" >
          I am studying a PhD in Intelligent Cinematography (Yr. 3/3.5). My research is focused on dynamic 3-D reconstruction from video, for cinematographic applications. 
          I have works on NeRFs and Gaussian Splatting for both dynamic scene and scene relighting tasks.
          The project is supervised by <a href="https://www.bristol.ac.uk/people/person/David-Bull-f53987d8-4d62-431b-8228-2d39f944fbfe/">Dave Bull</a>
          and <a href="https://www.bristol.ac.uk/people/person/Pui-Anantrasirichai-49b2250e-53e4-4631-aedc-8154b2cff568/">Pui Anantrasirichai</a> and funded by
          <a href="https://www.myworld-creates.com/">My World</a>.
        </p>
  
  
        <p class="introduction-text">
          I completed an MEng in Electronic Engineering with AI (First Class) at the <a href="https://www.ecs.soton.ac.uk/">University of Southampton</a>. My dissertation focused on describing philosophical, legal and social decision making frameworks with machine learning algorithms; supervised by
          <a href="https://www.ecs.soton.ac.uk/people/mjw7">Mark Weal</a>.
        </p>
      </div>

      <!-- <div class="blog-link-container">
        If you are looking for paper over-views or disucssions on Gaussian Splatting redirect <a href="./blog/index.html">here</a>.
      </div> -->
      
    </div>

    <hr>

    <div class="banner">
      <p class="banner-text">Research Papers</p>
    </div>

    <div class="project-container">
      
      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Splatography: Sparse multi-view dynamic Gaussian Splatting for film-making challenge (technical)</h1>
        <h2 style="margin-top:-10px"><a href="https://arxiv.org/abs/2312.02218">International Conference on 3D Vision 2026 (3DV)</a> - <a href="https://interims-git.github.io/">page</a> - <a href="https://github.com/azzarelli/Splatography-3DV-2026">code</a> - Jan-Aug 2025</h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. 
            However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. 
            To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks 
            for frames at t=0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are 
            modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are 
            learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 
            2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the 
            SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. 
          </p>
        </div>

      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields (technical)</h1>
        <h2 style="margin-top:-10px"><a href="https://arxiv.org/abs/2312.02218">arXiv</a> - <a href="./waveplanespage/index.html">page</a> - <a href="https://github.com/azzarelli/waveplanes/">code</a> - July-Nov 2024</h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model moving 3-D scenes. However, current methods are resource intensive and challenging to compress. 
            To address this, we present WavePlanes, a fast and more compact hex plane representation, applicable to both dynamic Neural Radiance Fields and Gaussian Splatting methods. 
            Rather than modeling many feature scales separately (as done previously), we use the inverse discrete wavelet transform to reconstruct features at varying scales. This 
            leads to a more compact representation and allows us to explore wavelet-based compression schemes for further gains. The proposed compression scheme exploits the sparsity 
            of wavelet coefficients, by applying hard thresholding to the wavelet planes and storing nonzero coefficients and their locations on each plane in a Hash Map. Compared to 
            the state-of-the-art (SotA), WavePlanes is significantly smaller, less resource demanding and competitive in reconstruction quality. Compared to small SotA models, WavePlanes 
            outperforms methods in both model size and quality of novel views.
          </p>
        </div>
      </span>
      
      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">ViVo: A Dataset for Volumetric Video Reconstruction and Compression (dataset)</h1>
        <h2 style="margin-top:-10px"><a href="">arXiv (TBD)</a> - <a href="https://vivo-bvicr.github.io/">page</a> - <a href="https://github.com/azzarelli/waveplanes/">code</a> - June-June 2024/5</h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            As research on neural volumetric video reconstruction and compression flourishes, there is a need for diverse and realistic datasets, which can be used to develop and validate 
            reconstruction and compression models. However, existing volumetric video datasets lack diverse content in terms of both semantic and low-level features that are commonly present 
            in real-world production pipelines. In this context, we propose a new dataset, \name, for VolumetrIc VideO reconstruction and compression. The dataset is faithful to 
            real-world volumetric video production and is the first dataset to extend the definition of diversity to include both human-centric characteristics (skin, hair, etc.) and 
            dynamic visual phenomena (transparent, reflective, liquid, etc.). Each video sequence in this database contains raw data including fourteen multi-view RGB and depth video pairs, 
            synchronized at 30FPS with per-frame calibration and audio data, and their associated 2-D foreground masks and 3-D point clouds. To demonstrate the use of this database, we have 
            benchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two volumetric video compression algorithms. The obtained results evidence the challenging nature of the 
            proposed dataset and the limitations of existing datasets for both volumetric video reconstruction and compression tasks, highlighting the need to develop more effective algorithms 
            for these applications.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Reviewing Intelligent Cinematography: AI research for camera-based video production (review; 41 pages)</h1>
        <h2 style="margin-top:-10px"><a href="https://link.springer.com/article/10.1007/s10462-024-11089-3">Springer Nature: Artificial Intelligence Review</a> - Sep-Apr 2022/25</h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            The first (comprehensive) review of computer vision research in the context of real video content acquisition for entertainment. 
            To establish a structure, we categorise work by General, Virtual, Live and Aerial production, and within each category we 
            discuss various machine learning applications and their links to other forms production. We also provide category-specific comments
            on future works and discuss the socail responsibilities for conducting ethical research.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Exploring Dynamic Novel View Synthesis Technologies for Cinematography (review/art application)</h1>
        <h2 style="margin-top:-10px"><a href="TBD">arXiv</a> - Mar-May 2024</h2>
      </div>
      <div class="project-body">
        <p style=" font-size: 13px;">
          We provide an overview of Dynamic NeRF and Gaussian Splatting research in the context of cinematography and explore
          the use of these technologies (Nerfacto, 4D-GS and SC-GS) to produce (very) short film. Topics discussed: 
          (1) Dynamic representations, (2) Articulated models vs Scene-based modelling, (3) Data collection
        </p>
      </div>

      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Towards a Robust Framework for NeRF Evaluation (technical)</h1>
        <h2 style="margin-top:-10px"><a href="https://arxiv.org/abs/2305.18079">arXiv</a> - <a href="https://github.com/azzarelli/wape">code</a> - Jan-May 2023</h2>
      </div>
      <div class="project-body">
        <p style="font-size: 13px;">
          Moving towards robust NeRF evaluation using synthetic datasets for point-based architectures. 
          This focuses on point-quality and biases involved with various training and test camera distributions to derive metrics for scene complexity.
        </p>
      </div>

      </span>
    </div>

    <hr>

    <div class="banner">
      <p class="banner-text" id="proj">UG Projects</p>
    </div>

    <div class="project-container">

      <span>
        <h1 class="subtitle">(Industry Collaboration) Online Trend Detection at Scale</h1>
        <h2 style="margin-top:-10px">MEng Group Design Project - Client: Senseye Ltd. - Sept-Jan 2021/22</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          We produced three unsupervised general trend detection models models which. My part focused on detection using variance-related errors. 
          This allowed for comprehensive evaluation of monotonic and heteroscedastic trends and 
          was capable of detecting various behaviours that classical and
          SoA approaches can not. Additionally, I investigated multi-scale detection for non-uniformly distributed data using a binary change-point detection algorithm to
          control the window size for evaluation. Documentation
          <a href="https://github.com/azzarelli/Trend-Detection-at-Scale">here</a>.
        </p>
      </span>
      <span>
        <h1 class="subtitle">(Thesis) Generalised Ethical Dilemma Solver</h1>
        <h2 style="margin-top:-10px">MEng Thesis - Sept-May 2020/21</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          By relating classical ML techniques to well defined philosophical, social and ethical frameworks
          I developed a machine learning algorithm to resolve casual social dilemas. Given a set of possible outcomes, this: (1) selects the appropriate philosophical/social/ethical
          model based on generally accepted criteria, (2) simulates each model and derive scores for the various resolutions, (3) applies a weighted fusion to make a final choice,
          and (4) provides reasoning for the choice by refering to the selected models and scores. Documentation
          <a href="https://github.com/azzarelli/MEng-Dissertation">here</a>.
        </p>
      </span>
      <span>
        <h1 class="subtitle">Solving a Maze blind using a Duelling Deep Q-Network</h1>
        <h2 style="margin-top:-10px">Model Implementation and Extension - Jan-June 2022 - Not Published</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          The problem: You are blind, in a maze and fires appear randomly arround you each time you take a step. Can we resolve the maze unsupervised?
          This looks at deep Q-learning approaches to solve the maze and compares results for Deep-Q, Duelling Deep-Q, Rainbow algorithms.
          Additionally, I investigated <i>combined experience replay</i> and compared the greedy episilon and Boltzmann eplxoration 
          methods. Code
          <a target="_blank" href="https://github.com/azzarelli/RL-Dynamic-Maze-Solver">here</a>.
          
        </p>
      </span>
      <span>
        <h1 class="subtitle">(Paper Extension) The Importance of Group-Size Preferences for the Evolution of Cooperation Under the Conditions of Individual Selection</h1>
        <h2 style="margin-top:-10px">Literature Review and Model Extension - Jan-May 2022 - Not Published</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          I reproduced and extended the proposed <i>Genetic Algorithm</i>; which focuses on replicating the conditions of
          grouping and dispersal in bacterial micro-colonies to provide insight into cooperative structures.
          The extension focused on better representing individuals to allow for imprecisions in their understanding of
          the simulated environment. Paper and code found
          <a target="_blank" href="https://github.com/azzarelli/Individual-Selection-for-Cooperative-Group-Formation/tree/main/coursework2">here</a>.
        </p>
      </span>
      
      <span>
        <h1 class="subtitle">(Paper Reproduction) Inspecting Functional Modularity in NNs</h1>
        <h2 style="margin-top:-10px">Group Project - Jan-June 2022 - Not Published</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          A reproduction of, <a href="https://arxiv.org/abs/2010.02066">Are Neural Nets Modular? Inspecting Functional Modularity through Differentiable Weight Masks</a> (ICLR '21).
          We reproduce the proposed tool for inspecting functional modularity and tested it on a range of NN architectures
          (namely CNNs and RNNs).
        </p>
      </span>

      <span>
        <h1 class="subtitle">(Internship) Improved Distribution of Physical Sensor Networks under Star Topology</h1>
        <h2 style="margin-top:-10px">Research Internship at <a href="https://lurtis.com/en/">Lurtis Rules</a> - July-Nov 2020  - Rejection</h2>
        <p style="margin-top:-10px; font-size: 13px;">
          We wrote a comparative short paper on evaluating various linear and non-linear strategies
          for multi-objective sensor distribution for general irregular (agricultural) fields; supervised by
          <a href="https://www.linkedin.com/in/jose-m-chema-pe%C3%B1a-a0944569/">Jose M. Pena</a>. This primarily uses unsupervised
          <i>differential evolution</i>.
        </p>
      </span>


    </div>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>

  </div>

  <div id="background">
    <span class="dot"></span>
    <div class="triangle"></div>
  </div>

  <div class="topbar">
    <div class="tabs">
      <div class="headingcontainers" style="width: 35%;">
        <p class="tab_txt">Adrian Azzarelli</p>
      </div>
      <div class="headingcontainers" style="width: 65%;">
        <p class="tab_txt"><a href="https://vilab.blogs.bristol.ac.uk/" style="text-decoration:none;color:black;">VI Labs, University of Bristol</a></p>
      </div>
    </div>
  </div>
  <div class="top_subbar">
    <div class="linktab" style="background-color: #0E768A;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="https://www.linkedin.com/in/adrian-azzarelli/">linked-in/</a></p>
    </div>
    <div class="linktab" style="background-color: #000000;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="https://github.com/azzarelli">github/</a></p>
    </div>
    <div class="linktab" style="background-color: #e30a0a;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="mailto:a.azzarelli@bristol.ac.uk">@bristol.ac.uk</a></p>
    </div>
    
  </div>

</div>

</body>
</html>
