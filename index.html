<html>
  
  <meta charset="UTF-8">
  <meta content="width=device-width, initial-scale=1" name="viewport" />

<head>
  <link rel="preload" href="scripting/home.js" as="script">

  <link href="./css/foreground.css" type="text/css" rel="stylesheet">
  <link href="./css/background.css" type="text/css" rel="stylesheet">
  
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">

  <script src="js/dropdown.js"></script>

</head>


<body style="padding-right: 0px;">
<script src="scripting/home/script.js"></script>
<script src="scripting/home/javaid.js"></script>

  <div id="foreground">
    <div class="banner" style="margin-top: 25px;">
      <p class="banner-text">Personal Introduction</p>
    </div>
    <div class="project-container" style="padding-bottom:0px;display: flex;">

      <div class="introduction-text-container">
        <p class="introduction-text" >
          I love digital paitning, animation and clothes making. 
          I also love reading research papers, designing neural architectures and developing digital tools for cinematography. 
          During my PhD, I had the opportunity to marry my passions, by researching computer vision tools for filmmaking. 
          Through this, I gained experience working of Virtual Production and 360 degree Metaverse filming stages and 
          collaborating with artists to develop digital tools and production workflows that solve real world problems 
          and/or propose new approaches to content creation. Thinking about the future, I would like continue pursuing
          my passion for digital tooling for artistic applications.

          <br>
          <br>
          If you are curious about my work, looking for gaps in research or interested to hire/collaborate with me
          feel free to email me: aazzarellib@gmail.com

          <br>
          <br>
          To download my CV <a href="src/adrianazzarelli_26.pdf">click here!</a>
        </p>
      </div>
    </div>

    <hr>

    <div class="banner">
      <p class="banner-text">Education</p>
    </div>

    <div class="project-container">
      
      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Intelligent Cinematography, PhD</h1>
        <h2 style="margin-top:-10px">Sep-Mar 2022/26 | Visual Information Lab, Universtiy of Bristol
          <br> Funded by <a href="https://www.myworld-creates.com/">MyWorld</a> and UKRI 
          <br> Supervised by  <a href="https://www.bristol.ac.uk/people/person/David-Bull-f53987d8-4d62-431b-8228-2d39f944fbfe/">Dave Bull</a>
          and <a href="https://www.bristol.ac.uk/people/person/Pui-Anantrasirichai-49b2250e-53e4-4631-aedc-8154b2cff568/">Pui Anantrasirichai</a>  </h2>
      </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            <i>Intelligent Cinematography</i> applies machine learning to real in-camera content acquisition for filmmaking.
            Throughout the 3.5 years of PhD, my focus was on adapting 3-D reconstruction techniques to filmmaking scenarios.
            I explored various topics, from efficient/compact dynamic 3-D reconstruction, sparse multi-view camera problems 
            and relighting real scenes in Virtual Production scenarios. (see "Research" section for specific topics).
            <br><br>
            <b>Key words:</b> Neural Radiance Fields, 
            Local Light Fields, Gaussian Splatting, Differentiable Rendering, Dynamic 3-D, Relightable 3-D Content.
            <br><br>
            <b>Industry collaborations:</b> <a href="https://condensereality.com/">Condense Reality</a>, <a href="https://www.luxaeterna.com/">LuxAeterna</a>.

          </p>
        </div>

      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Electronic Engineering With AI, MEng</h1>
        <h2 style="margin-top:-10px">Sep-Sep 2018/22 | Universtiy of Southampton
          <br> Thesis supervised by <a href="https://www.ecs.soton.ac.uk/people/mjw7">Mark Weal</a> </h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            My thesis <i>A Decision Making Framework for Generalized Philosophical, Social and Legal Dilemma</i> explores a multi-model framework
            for assessing and resolving complex social decisions. I developed algorithms based on the philosophy of logic applied to philosophical, social and legal
            dilemma. This thesis was graded 86% (high First Class for UK grading) and recieved the title of "award worthy". 
            <br><br>
            <b>Relevant Modules:</b> (graded First Class) 
            <br>
            Advanced Programming, Robotic Systems, Advanced Image Processing, Numerical Methods, Mathematcal Optimization,
            Reinforcement and Online Learning, Engineering Management and Law 
            <br><br>
            <b>Co-Curricular:</b>
            <br> Student Faculty President 2020 & 21(COVID period) 
            <br> Head of the Electronic and Computer Science Mentoring Schee, 2020 & 2021 (COVID period) 
          </p>
        </div>

      </span>

    </div>

    <hr>

    <div class="banner">
      <p class="banner-text">Languages and Libraries</p>
    </div>

    <div class="project-container">
      
      <span class="project">
      <div class="project-header">
        <!-- <h1 class="subtitle">Intelligent Cinematography, PhD</h1> -->
        <h2 style="margin-top:0px;margin-bottom: 0px;">
        <b>Spoken Languages:</b> English, French and Italian
        </h2>
      </div>
      <div class="project-body">
          <p style=" font-size: 13px;">
            I am French and Italian, and grew up in London, UK. I can speak English and French with high profficients. My Italian
            is casual.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <!-- <h1 class="subtitle">Intelligent Cinematography, PhD</h1> -->
        <h2 style="margin-top:0px;margin-bottom: 0px;">
        <b>Computer Languages:</b> Python, C++, C# (with Unity), embedded-C, HSLS, Julia, JavaScript, MatLab
        </h2>
      </div>
      <div class="project-body">
          <p style=" font-size: 13px;">
            My MEng required me to learn obj-C, C++, Python and MatLab applied to various domains including embedded systems, image
            processing and robotics. My PhD required further developed my Python skills. I have also engaged in some 
            personal projects (e.g. following <a href="https://catlikecoding.com/unity/tutorials/">CatLike Tutorials</a>) 
            to develop my understanding of C#, HSLS and C++ for graphics applications. I have also tinkered with Julia, JS and other
            languages for some personal tools.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <!-- <h1 class="subtitle">Intelligent Cinematography, PhD</h1> -->
        <h2 style="margin-top:0px;margin-bottom: 0px;">
        <b>Software/Libraries:</b> Blender, Unity, Unreal Engine, Procreate, ClipStudioPaint, Photoshop, DaVinci Resolve,
        CUDA/Optix (RTX code), PyTorch, Numpy/SciPy/CuPy, Python-C++ wrappers   
        </h2>
      </div>
      <div class="project-body">
          <p style=" font-size: 13px;">
            I have experience using Blender, Unity, Unreal, Procreate CSP, Photoshop and DaVinci for personal artistic projects.
            Whether its for a cool effect or smooth animation, I love exploring new ways to develop my artistic abilities. 
            I also have work/coding experience with Blender-Python, Unreal-C++ and Unity-C#. I also have work experience
            with CUDA/OPTIX, PyTorch and most other python-based libraries.
          </p>
        </div>
      </span>


    </div>

    <hr>

    <div class="banner">
      <p class="banner-text">Research Projects</p>
    </div>

    <div class="project-container">
      <span class="project">
      <div class="project-header">
        <h1 id="vsr" class="subtitle">Gaussian Splatting Light Simulation for Virtual Production</h1>
        <h2 style="margin-top:-10px">2025/6 | [Submitted to SIGGRAPH] Please email for link to videos and paper
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
This paper investigates 3D scene reconstruction and relighting for virtual production (VP) stages, where real foreground objects are captured in front of LED walls that display virtual backgrounds and provide real image-based lighting (IBL) to the foreground. However, this fixes the scene's appearance at capture time and as VP stage lighting is unique to each set it is difficult to anticipate the scene's appearance. So, when the footage does not match the director's expectations re-shoots are required. These are uniquely expensive due to the cost and logistical complexity of VP stages. Addressing this, we propose a VP 3D reconstruction and relighting (VSR) pipeline that synthesizes photorealistic 3D twins of VP scenes and enables changing LED wall content while propagating the corresponding lighting effects to the foreground.
<br><br>
Our VSR method relies on multi-view images of a static scene including a real in-camera IBL background, captured under variable background and lighting conditions - a capture setting for which no prior reconstruction or relighting method exists. We therefore establish a foundation for VSR by introducing a novel Gaussian Splatting (GS)-based pipeline, several proxy baselines and multiple datasets including real VP captures. The main technical problems concern designing a suitable representation and re-lighting scheme that samples IBL textures to change the scene's lighting on a per-primitive basis. The main contribution is a geometry-independent GS lighting model that represents IBL texture sampling coordinates and lighting intensity as view-dependent GS parameters. As this model avoids ray-based inverse rendering, it does not require depth or normal priors to learn transmission and reflection effects, it supports complex scenes including transparent objects, and it is implemented without custom CUDA or RTX code. This resulting representation is compact and efficient, requiring less than 5GB of RAM or VRAM to train 1080p scenes. In practice, our approach reduces the reliance on VP-specific hardware and enables greater creative flexibility in post-production. The code, datasets, documentation and video results are available online.
          </p>
        </div>

      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Splatography: Sparse multi-view dynamic Gaussian Splatting for film-making challenge (technical)</h1>
        <h2 style="margin-top:-10px">2025 | 
          International Conference on 3D Vision 2026 (3DV) - <a href="https://arxiv.org/abs/2312.02218">paper</a> - 
          <a href="./splatographypage/index.html">page</a> - <a href="https://github.com/azzarelli/Splatography-3DV-2026">code</a></h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. 
            However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. 
            To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks 
            for frames at t=0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are 
            modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are 
            learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 
            2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the 
            SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. 
          </p>
        </div>

      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">ViVo: A Dataset for Volumetric Video Reconstruction and Compression (dataset)</h1>
        <h2 style="margin-top:-10px">
          2024/5 | Special Issue on Volumetric Video Reconstruction and Compression, T-CSVT, IEEE 2026 - <a href="https://arxiv.org/abs/2506.00558">paper</a> - <a href="https://vivo-bvicr.github.io/">page</a>
        </h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            As research on neural volumetric video reconstruction and compression flourishes, there is a need for diverse and realistic datasets, which can be used to develop and validate 
            reconstruction and compression models. However, existing volumetric video datasets lack diverse content in terms of both semantic and low-level features that are commonly present 
            in real-world production pipelines. In this context, we propose a new dataset, \name, for VolumetrIc VideO reconstruction and compression. The dataset is faithful to 
            real-world volumetric video production and is the first dataset to extend the definition of diversity to include both human-centric characteristics (skin, hair, etc.) and 
            dynamic visual phenomena (transparent, reflective, liquid, etc.). Each video sequence in this database contains raw data including fourteen multi-view RGB and depth video pairs, 
            synchronized at 30FPS with per-frame calibration and audio data, and their associated 2-D foreground masks and 3-D point clouds. To demonstrate the use of this database, we have 
            benchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two volumetric video compression algorithms. The obtained results evidence the challenging nature of the 
            proposed dataset and the limitations of existing datasets for both volumetric video reconstruction and compression tasks, highlighting the need to develop more effective algorithms 
            for these applications.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields (technical)</h1>
        <h2 style="margin-top:-10px">2023 | arXiv - <a href="https://arxiv.org/abs/2312.02218">paper</a> - <a href="./waveplanespage/index.html">page</a> - <a href="https://github.com/azzarelli/waveplanes/">code</a></h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model moving 3-D scenes. However, current methods are resource intensive and challenging to compress. 
            To address this, we present WavePlanes, a fast and more compact hex plane representation, applicable to both dynamic Neural Radiance Fields and Gaussian Splatting methods. 
            Rather than modeling many feature scales separately (as done previously), we use the inverse discrete wavelet transform to reconstruct features at varying scales. This 
            leads to a more compact representation and allows us to explore wavelet-based compression schemes for further gains. The proposed compression scheme exploits the sparsity 
            of wavelet coefficients, by applying hard thresholding to the wavelet planes and storing nonzero coefficients and their locations on each plane in a Hash Map. Compared to 
            the state-of-the-art (SotA), WavePlanes is significantly smaller, less resource demanding and competitive in reconstruction quality. Compared to small SotA models, WavePlanes 
            outperforms methods in both model size and quality of novel views.
          </p>
        </div>
      </span>
      
      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Reviewing Intelligent Cinematography: AI research for camera-based video production (review)</h1>
        <h2 style="margin-top:-10px">2022/5 | Springer Nature: Artificial Intelligence Review, Vol 58 Issue 4 (+40 pages) - <a href="https://link.springer.com/article/10.1007/s10462-024-11089-3">paper</a></h2>
        </div>
        <div class="project-body">
          <p style=" font-size: 13px;">
            The first (comprehensive) review of computer vision research in the context of real video content acquisition for entertainment. 
            To establish a structure, we categorise work by General, Virtual, Live and Aerial production, and within each category we 
            discuss various machine learning applications and their links to other forms production. We also provide category-specific comments
            on future works and discuss the socail responsibilities for conducting ethical research.
          </p>
        </div>
      </span>

      <span class="project">
      <div class="project-header">
        <h1 class="subtitle">Exploring Dynamic Novel View Synthesis Technologies for Cinematography (review & application)</h1>
        <h2 style="margin-top:-10px">2024 | arXiv - <a href="https://arxiv.org/abs/2412.17532">paper</a></h2>
      </div>
      <div class="project-body">
        <p style=" font-size: 13px;">
          We provide an overview of Dynamic NeRF and Gaussian Splatting research in the context of cinematography and explore
          the use of these technologies (Nerfacto, 4D-GS and SC-GS) to produce (very) short film. Topics discussed: 
          (1) Dynamic representations, (2) Articulated models vs Scene-based modelling, (3) Data collection
        </p>
      </div>

      </span>

    </div>

    <hr>

    <div class="banner">
      <p class="banner-text" id="proj">3-D Video Examples</p>
    </div>

    <div class="project-container">
      <!-- <video poster="" class="videoDisplay" autoplay controls muted loop height="100%">
        <source src="src/showcase.mp4"
        type="video/mp4">
      </video>
      <h2 class="videoInfo">
        An example of relighting Virtual Production content with our <a href="https://interims-git.github.io/">Gaussian Splatting pipeline</a>
      </h2>
      <iframe 
        width="100%" 
        height="480"
  src="https://www.youtube.com/embed/z8YZhX7JfDU"
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        allowfullscreen>
      </iframe>
      <h2 class="videoInfo">
        I streamed a background I was designing in Photoshop to our <a href="https://interims-git.github.io/">Gaussian Splatting reconstruction and relighting pipeline</a>
        for Virtual Productio to showcase the future potential of creative film making.
      </h2>
      <video
        class="videoDisplay"
        controls
        preload="metadata"
        playsinline
      >
        <source src="src/exposure.webm" type="video/webm">
        Your browser does not support the video tag.
      </video>
      <h2 class="videoInfo">
        I developed IBL exposure and recoloring settings for our <a href="https://interims-git.github.io/">Gaussian Splatting reconstruction and relighting pipeline</a>
        for Virtual Productio to showcase the future potential of creative film making.
      </h2>
       -->
       <video poster="" class="videoDisplay" autoplay controls muted loop height="100%">
          <source src="splatographypage/static/videos/piano_vfx.mp4"
          type="video/mp4">
        </video>
        <h2 class="videoInfo">
          After developing a pipeline for <a href="https://github.com/azzarelli/Splatography-3DV-2026">reconstructing 3-D scenes from sparse multi-view camera set ups</a>
          I implemented various VFX tools for Gaussian Splatting representations (wait for the end).
        </h2>

        <video poster="" class="videoDisplay" autoplay controls muted loop height="100%">
          <source src="splatographypage/static/videos/showcase.mp4"
          type="video/mp4">
        </video>
        <h2 class="videoInfo">
          Same as above but this time I made an audio-visual effect for Gaussian Splatting
        </h2>


    </div>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>

  </div>

  <div id="background">
  </div>

  <div class="topbar">
    <div class="tabs">
      <div class="headingcontainers" style="width: 35%;">
        <p class="tab_txt">Adrian Azzarelli</p>
      </div>
      <div class="headingcontainers" style="width: 65%;">
        <p class="tab_txt"><a href="https://vilab.blogs.bristol.ac.uk/" style="text-decoration:none;color:black;">VI Labs, University of Bristol</a></p>
      </div>
    </div>
  </div>
  <div class="top_subbar">
    <div class="linktab" style="background-color: #0E768A;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="https://www.linkedin.com/in/adrian-azzarelli/">linked-in/</a></p>
    </div>
    <div class="linktab" style="background-color: #000000;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="https://github.com/azzarelli">github/</a></p>
    </div>
    <div class="linktab" style="background-color: #e30a0a;">
      <p class="link_txt"><a class="link_link" style="color: #ffffff;" href="mailto:a.azzarelli@bristol.ac.uk">@bristol.ac.uk</a></p>
    </div>
    
  </div>

</div>

</body>
</html>
